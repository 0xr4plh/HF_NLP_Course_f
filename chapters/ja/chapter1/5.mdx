# Encoder models / エンコーダーモデル

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="MUqNwgPjJvQ" />

Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having "bi-directional" attention, and are often called *auto-encoding models*.

The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.

Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.

Representatives of this family of models include:

エンコーダーモデルとは、Transformerモデルのエンコーダーのみを使用したモデルを指します。 処理の各段階で、attention層は最初の文の全ての単語にアクセスすることができます。 これらのモデルは "bi-directional"（双方向）のattentionを持つものとして特徴付けられ、*オートエンコーダーモデル*と呼ばれます。

これらのモデルの事前学習は、何らかの方法で（例えば文中の単語をランダムにマスクするなどで）文を壊し、この文の再構築をタスクとして解くことを中心に展開されます。

エンコーダーモデルは、文の分類 ・ 固有表現認識（より一般的には単語の分類） ・ 抽出的質問応答など、文全体の理解を必要とするタスクに最も適しています。

エンコーダーモデルでは以下のものが代表的です：

- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)
- [BERT](https://huggingface.co/transformers/model_doc/bert.html)
- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)
- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)
- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
