# Decoder models / デコーダーモデル

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="d_ixlCubqQw" />

Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called *auto-regressive models*.

The pretraining of decoder models usually revolves around predicting the next word in the sentence.

These models are best suited for tasks involving text generation.

Representatives of this family of models include:

デコーダーモデルとは、Transformerモデルのデコーダーのみを使用したモデルを指します。 処理の各段階で、処理対象の単語について、attention層はその単語より前に出現した単語にのみアクセスすることができます。 このようなモデルは*自己回帰モデル*と呼ばれます。

　デコーダーモデルの事前学習は、次に続く単語を予測するタスクを解くことを中心に展開されます。

これらのモデルは、文を生成するタスクに最も適しています。


デコーダーモデルでは以下のものが代表的です：

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)
- [GPT](https://huggingface.co/transformers/model_doc/gpt.html)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transfo-xl.html)
