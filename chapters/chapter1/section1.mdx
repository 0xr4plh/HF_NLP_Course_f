<script>
	import Question from "../../Question.svelte";
	import Tip from "../../Tip.svelte";
	import Youtube from "../../Youtube.svelte";
	
	export let fw: "pt" | "tf";
</script>

## Welcome to the ðŸ¤— Course!

<Youtube id="7PhlevizVB4" />

This course will teach you about natural language processing (NLP) using libraries from the [Hugging Face](https://huggingface.co/) ecosystem â€” [ðŸ¤— Transformers](https://github.com/huggingface/transformers), [ðŸ¤— Datasets](https://github.com/huggingface/datasets), [ðŸ¤— Tokenizers](https://github.com/huggingface/tokenizers), and [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate) â€” as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and without ads.


## What to expect?

Here is a brief overview of the course:

<img src="/course/static/chapter1/summary.png" alt="Brief overview of the chapters of the course." width="100%">

- Chapters 1 to 4 provide an introduction to the main concepts of the ðŸ¤— Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the [Hugging Face Hub](https://huggingface.co/models), fine-tune it on a dataset, and share your results on the Hub!
- Chapters 5 to 8 teach the basics of ðŸ¤— Datasets and ðŸ¤— Tokenizers before diving into classic NLP tasks. By the end of this part, you will be able to tackle the most common NLP problems by yourself.
- Chapters 9 to 12 dive even deeper, showcasing specialized architectures (memory efficiency, long sequences, etc.) and teaching you how to write custom objects for more exotic use cases. By the end of this part, you will be ready to solve complex NLP problems and make meaningful contributions to ðŸ¤— Transformers.

This course:

* Requires a good knowledge of Python
* Is better taken after an introductory deep learning course, such as [Practical Deep Learning for Coders](https://course.fast.ai/) or the courses developed by [deeplearning.ai](https://www.deeplearning.ai/)
* Does not expect prior [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/) knowledge, though some familiarity with either of those will help

## Who are we?

About the authors:

**Matthew Carrigan** is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we're going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless.

**Lysandre Debut** is a Machine Learning Engineer at Hugging Face and has been working on the ðŸ¤— Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API.

**Sylvain Gugger** is a Research Engineer at Hugging Face and one of the core maintainers of the ðŸ¤— Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources.

Are you ready to roll? In this chapter, you will learn:
* How to use the `pipeline` function to solve NLP tasks such as text generation and classification
* About the Transformer architecture
* How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases
