Replace this with a quiz

<!-- Now that we know how to build and customize a gradio interface, let's walk through the code of some popular Hugging Face spaces:

### The first model we'll walk through is AnimeGanV2. you can try the model [here](https://huggingface.co/spaces/akhaliq/AnimeGANv2).

<p align="center">
<img src="/course/static/chapter9/anime-gan.png" alt="An image showing the gradio interface for the anime gan model" width="80%"/>
</p>

```
import os
os.system("pip install gradio==2.8.0b3")
from PIL import Image
import torch
import gradio as gr

model2 = torch.hub.load(
    "AK391/animegan2-pytorch:main",
    "generator",
    pretrained=True,
    device="cuda",
    progress=False
)

model1 = torch.hub.load("AK391/animegan2-pytorch:main", "generator", pretrained="face_paint_512_v1",  device="cuda")

face2paint = torch.hub.load(
    'AK391/animegan2-pytorch:main', 'face2paint', 
    size=512, device="cuda",side_by_side=False
)

def inference(img, ver):
    if ver == 'version 2 (ðŸ”º robustness,ðŸ”» stylization)':
        out = face2paint(model2, img)
    else:
        out = face2paint(model1, img)
    return out
  
title = "AnimeGANv2"
description = "Gradio Demo for AnimeGanv2 Face Portrait. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below. Please use a cropped portrait picture for best results similar to the examples below."
article = "<p style='text-align: center'><a href='https://github.com/bryandlee/animegan2-pytorch' target='_blank'>Github Repo Pytorch</a></p> <center><img src='https://visitor-badge.glitch.me/badge?page_id=akhaliq_animegan' alt='visitor badge'></center></p>"
examples=[['groot.jpeg','version 2 (ðŸ”º robustness,ðŸ”» stylization)'],['bill.png','version 1 (ðŸ”º stylization, ðŸ”» robustness)'],['tony.png','version 1 (ðŸ”º stylization, ðŸ”» robustness)'],['elon.png','version 2 (ðŸ”º robustness,ðŸ”» stylization)'],['IU.png','version 1 (ðŸ”º stylization, ðŸ”» robustness)'],['billie.png','version 2 (ðŸ”º robustness,ðŸ”» stylization)'],['will.png','version 2 (ðŸ”º robustness,ðŸ”» stylization)'],['beyonce.png','version 1 (ðŸ”º stylization, ðŸ”» robustness)'],['gongyoo.jpeg','version 1 (ðŸ”º stylization, ðŸ”» robustness)']]
gr.Interface(inference, [gr.inputs.Image(type="pil"),gr.inputs.Radio(['version 1 (ðŸ”º stylization, ðŸ”» robustness)','version 2 (ðŸ”º robustness,ðŸ”» stylization)'], type="value", default='version 2 (ðŸ”º robustness,ðŸ”» stylization)', label='version')
], gr.outputs.Image(type="pil"),title=title,description=description,article=article,examples=examples,allow_flagging=False,allow_screenshot=False).launch(enable_queue=True,cache_examples=True)
```

- First, we pip install the gradio package
- Next we are use torch hub to download and load the models for face_paint_512_v1 and face_paint_512_v2
- Then we define our inference function which takes in a PIL image and a radio button (list of strings with options to select from) to switch between the two models
- We have an if statement to check which model is selected for the radio button, if version 2 is selected, we use model2 and pass our input image to the face2paint function, same for model1, then we return the output PIL image
- We add our inference function, input component, output component (both pil image types), title, description, article and examples to the Gradio interface
- Finally, we then turning on cache for examples and queueing and launch our app


### The second model we'll walk through is Wav2lip. you can try the model [here](https://huggingface.co/spaces/PaddlePaddle/wav2lip).

<p align="center">
<img src="/course/static/chapter9/wav2lip.png" alt="An image showing the gradio interface for the Wav2lip model" width="80%"/>
</p>

```
import os
os.system("python -m pip install paddlepaddle-gpu==2.2.1.post112 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html")
os.system("hub install wav2lip==1.0.0")
import gradio as gr
import paddlehub as hub

module = hub.Module(name="wav2lip")

def inference(image,audio):
    module.wav2lip_transfer(face=image, audio=audio, output_dir='.', use_gpu=False)  
    return "result.mp4"
    
title = "Wav2lip"
description = "Gradio demo for Wav2lip: Accurately Lip-syncing Videos In The Wild. To use it, simply upload your image and audio file, or click one of the examples to load them. Read more at the links below. Please trim audio file to maximum of 3-4 seconds"

article = "<p style='text-align: center'><a href='https://arxiv.org/abs/2008.10010' target='_blank'>A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</a> | <a href='https://github.com/PaddlePaddle/PaddleGAN/blob/develop/docs/en_US/tutorials/wav2lip.md' target='_blank'>Github Repo</a></p>"
examples=[['monatest.jpeg',"game.wav"]]
iface = gr.Interface(inference, [gr.inputs.Image(type="filepath"),gr.inputs.Audio(source="microphone", type="filepath")], 
outputs=gr.outputs.Video(label="Output Video"),examples=examples,title=title,article=article,description=description)
iface.launch(cache_examples=True,enable_queue=True)
```

- Again, the first step here is to pip install the dependencies (gradio, paddlepaddle etc)
- Next we download and load the wav2lip model from paddle hub
- We then define our inference function which takes in a image from a file path and audio through the microphone from a file path
- We pass in the image and audio paths to the wav2lip_transfer function along with setting a output directory for videos and setting gpu to false.
- We return the output video filepath for the Output Video Component
- The gradio interface then has the inference function, input component for images and audio as a list, output component for Videos, examples, article, description and a title
- Finally, we launch our app and enable the queue and cache examples -->