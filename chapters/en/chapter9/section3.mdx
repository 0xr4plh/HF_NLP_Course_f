In this section, we will take a closer look at the `Interface` class, and understand the
main parameters used to create an `Interface`.

## How to create an `Interface`

You'll notice that the `Interface` class has 3 required parameters:  

 `gradio.Interface(fn, inputs, outputs, ...)`

These parameters are:

  - `fn`: the core prediction function that is wrapped by the gradio interface. This function can take one or more parameters and return one or more values
  - `inputs`: the input component type(s) (`gradio` provides many pre-built components e.g. `"image"` or `"mic"`). 
  - `outputs`: the output component type(s)  (again, `gradio` provides many pre-built components e.g. `"image"` or `"label"`).


For a complete list of components, [see the docs ](https://gradio.app/docs). Each pre-built
can be customized by instantiating the class corresponding to the component. 

For example,
instead of passing in `"textbox"` to the `inputs` parameter, you can pass in `gradio.inputs.Textbox(lines=7, label="Prompt")` to create a textbox with 7 lines and a label.

This will be clearer with an example.

## An example with images

As mentioned earlier, `gradio` provides many different inputs and outputs. So let's build an `Interface` that works with images.

In this example, we're building an image-to-image function that takes an uploaded image and applies a sepia filter. 

We will use for the input the `Image` component . 
When using the `Image` component, `gradio` automatically converts the uploaded image to a numpy array of your specified size, with the shape `(width, height, 3)`. 
In this case, we will specify the height to be 200 by 200 pixels.

We will also use the `Image` output component which can automatically
render a numpy array as an image. In this case, we do not need to do 
any customization, so we will use the string shortcut `"image"`.


```py
import numpy as np
import gradio as gr

def sepia(input_img):
  sepia_filter = np.array(
    [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]
  )
  sepia_img = input_img.dot(sepia_filter.T)
  sepia_img /= sepia_img.max()
  return sepia_img

iface = gr.Interface(fn=sepia, 
                     inputs=gr.inputs.Image(shape=(200, 200)), 
                     outputs="image")
iface.launch()
```
The code above will produce an interface like the one below:

<iframe src="https://hf.space/gradioiframe/abidlabs/sepia/+" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Notice that our `Image` input interface comes with an 'edit' button which opens tools for cropping, flipping, rotating, drawing over, and applying noise to images. Try playing around with it; it can be useful in exploring and revealing failure points of machine learning models.

## Multiple inputs and outputs:

Let's say we had a more complicated function, with multiple inputs and outputs. 
In the example below, we have a function that takes a string, boolean, and number, 
and returns a string and number. Take a look how we pass a list of input and output components.

In the list of input components, each component corresponds to a parameter in order.
In the list of output coponents, each component corresponds to a returned value.

```py
import gradio as gr

def greet(name, is_morning, temperature):
  salutation = "Good morning" if is_morning else "Good evening"
  greeting = f"{salutation} {name}. It is {temperature} degrees today"
  celsius = (temperature - 32) * 5 / 9
  return greeting, round(celsius, 2)

iface = gr.Interface(
  fn=greet,
  inputs=["text", "checkbox", gr.inputs.Slider(0, 100)],
  outputs=["text", "number"],
)
iface.launch()
```

<iframe src="https://hf.space/gradioiframe/abidlabs/salutations/+" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


### The `launch()` method

So far, we have used the `launch()` method to launch the interface, but we 
haven't really discussed what it does. 

By default, the `launch()` method will launch the demo in a web server that 
is running locally. If you are running your code in a Jupyter or colab notebook, then
`gradio` will embed the demo GUI in the notebook so you can easily use it.

You can customize the behavior of `launch()` through different parameters

  - `inline` - whether to display in the interface inline on python notebooks.
  - `inbrowser` - whether to automatically launch the interface in a new tab on the default browser.
  - `share` - whether to create a publicly shareable link from your computer for the interface. Kind of like a Google Drive link!

We'll cover the `share` parameter in a lot more detail in the next section!

## ✏️ Let's apply it!

We'll build an interface that allows you to demo a **speech-recognition** model.
To make it interesting, we will accept *either* a mic input or an uploaded file.

We'll load our speech recognition model using the the `pipeline` abstraction from the `transformers` library. 
If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/section3).   

```py
from transformers import pipeline
import gradio as gr

model = pipeline("automatic-speech-recognition")

def transcribe_audio(mic=None, file=None):
  if mic is not None:
    audio = mic
  elif file is not None:
    audio = file
  else:
    return("You must either provide a mic recording or a file")
  transcription = model(audio)["text"]
  return transcription

gr.Interface(
  fn=transcribe_audio,
  inputs=[gr.inputs.Audio(source="microphone", type="filepath", optional=True),
          gr.inputs.Audio(source ="upload", type="filepath", optional=True)],
  outputs="text",
).launch()
```

That's it! You can now use this interface to transcribe audio.

Keep going to see how to share your interface with others!
