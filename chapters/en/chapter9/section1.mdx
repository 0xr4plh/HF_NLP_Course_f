In this chapter we will be learning about how to build web-based demos for your machine learning models. 

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- Machine learning developers to easily present their work to a wide audience
- machine learnign researchers to more easily reproduce machine learning models and behavior
- Quality testers to more easily identify and debug failure points of models
- Diverse users to discover algorithmic biases in models

We'll be using the `gradio` Python package to build a demo for our model. The `gradio` package allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with `gradio`:

* A **sketch recognition** model that takes in a sketch and outputs labels of what it thinks is being drawn: 

<iframe src="https://hf.space/gradioiframe/abidlabs/draw2/+" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

* A **question answering** model that takes in a context paragraph and a quest and outputs a response and a probability: 

<iframe src="https://hf.space/gradioiframe/abidlabs/question-answering-simple/+" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

* A **voice verification** model that takes in 2 mic recordings and outputs whether they belong to the same person:

<iframe src="https://hf.space/gradioiframe/abidlabs/same-person-or-different/+" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


*Tip:* Check out the [Keras Org] (https://huggingface.co/keras-io) on Hugging Face to see many more examples of machine learning demos you can build.

<p align="center">
<img src="/course/static/chapter9/keras-org.gif" alt="A gif showing the keras org on Hugging Face Spaces" width="80%"/>
</p>
