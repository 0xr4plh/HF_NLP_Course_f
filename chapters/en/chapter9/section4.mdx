This section will cover sharing a demo that you have built. Before you
share your demo, you'll likely want to polish it up...

### Adding more content to your gradio interface:
  - Now you might notice that our interface looks too basic and plain. How can we add more context about our machine learning model? We can use the parameters below:
      - `title`: a title for the interface; if provided, appears above the input and output components.
      - `description`: - a description for the interface; if provided, appears above the input and output components.
      - `article`: - an expanded article explaining the interface; if provided, appears below the input and output components. Accepts Markdown and HTML content
      - `theme`: - Theme to use - one of `default`, `huggingface`, `grass`, `peach`. Add `dark-` prefix, e.g. `dark-peach` for dark theme (or just `dark` for the default dark theme).
      - `examples`: - sample inputs for the function; if provided, appears below the UI components and can be used to populate the interface. Should be nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. A string path to a directory of examples can also be provided. If there are multiple input components and a directory is provided, a log.csv file must be present in the directory to link corresponding inputs.
        
      ```
      gradio.Interface(self, fn, inputs=None, outputs=None, examples=None, theme=None, title=None, description=None, article=None)
      ```
        
Using these options, we end up with a more complete interface like the one below:

<iframe src="https://hf.space/gradioiframe/akhaliq/BlendGAN/+" frameBorder="0" height="900" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

### Sharing Gradio Interfaces
Now that we have a working demo of our machine learning model, let's learn how to easily share a link to our interface.
Interfaces can be easily shared publicly by setting `share=True` in the `launch()` method.
        
```
gr.Interface(classify_image, "image", "label").launch(share=True)
```
        
This generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on!), you don't have to worry about any packaging any dependencies. If you're working out of a Google colab notebook, a share link is always automatically created. It usually looks something like this: **XXXXX.gradio.app**. Although the link is served through a gradio link, we are only a proxy for your local server, and do not store any data sent through the interfaces.

Keep in mind, however, that these links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. If you set `share=False` (the default), only a local link is created, which can be shared by **[port-forwarding](https://www.ssh.com/ssh/tunneling/example)** with specific users.

### Uploading to Hugging Face Spaces

A share link that you can pass around to collegues is cool, but how can you permanently host your demo and have it exist in its own "space" on the internet?

Huggingface provides the infrastructure to permanently host your Gradio model on the internet, for free! Spaces allows you to create and push to a repo, where your gradio interface code will exist in an `app.py` file. See [Huggingface Spaces](https://huggingface.co/spaces) for more information.


## ✏️ Let's apply it!

Using what we just learned, let's create the sketch recognition demo we saw in [section one of this chapter](/course/chapter9/section1). Let's add some customization to our interface and set `share=True` to create a public link we can pass around.

We can load the labels from [class_names].txt(https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/class_names.txt) and load the pre-trained pytorch model from [pytorch_model.bin](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/pytorch_model.bin).

```py
from pathlib import Path

import torch
import gradio as gr
from torch import nn


LABELS = Path('class_names.txt').read_text().splitlines()

model = nn.Sequential(
    nn.Conv2d(1, 32, 3, padding='same'),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(32, 64, 3, padding='same'),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(64, 128, 3, padding='same'),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(1152, 256),
    nn.ReLU(),
    nn.Linear(256, len(LABELS)),
)
state_dict = torch.load('pytorch_model.bin', map_location='cpu')
model.load_state_dict(state_dict, strict=False)
model.eval()

def predict(im):
    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.

    with torch.no_grad():
        out = model(x)

    probabilities = torch.nn.functional.softmax(out[0], dim=0)

    values, indices = torch.topk(probabilities, 5)

    return {LABELS[i]: v.item() for i, v in zip(indices, values)}


interface = gr.Interface(
    predict, 
    inputs='sketchpad', 
    outputs='label', 
    theme="huggingface", 
    title="Sketch Recognition", 
    description="Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!", 
    article = "<p style='text-align: center'>Sketch Recognition | Demo Model</p>",
    live=True)
interface.launch(share=True)
```

<iframe src="https://hf.space/gradioiframe/dawood/Sketch-Recognition/+" frameBorder="0" height="650" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


We trained and built a sketch recognition demo with the above code; we also added customization to add context to our interface.

In section 5 we will go over more advanced features of gradio.
