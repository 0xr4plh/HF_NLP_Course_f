Now that you've built a demo, you'll probably want to share it with others. Gradio demos
can be shared in two ways, using a ***temporary share link*** or ***permanent hosting on Spaces***.

We'll cover both of those approach shortly. But before you share your demo, you mway want to polish it up... 

### Polishing your `gradio` demo:
  - Now you might notice that our interfaces looks too basic and plain. How can we add more context about our machine learning model? The `Interface` class supports some optional parameters:

      - `title`: you can give a title to your demo, which appears above the input and output components.
      - `description`: you can give a description (in text, Markdown, or HTML) for the interface, which appears above the input and output components and below the title.
      - `article`: you can also write an expanded article (in text, Markdown, or HTML) explaining the interface; if provided, appears below the input and output components.
      - `theme`: don't like the default colors? Set the theme to use one of `default`, `huggingface`, `grass`, `peach`. You can also add the `dark-` prefix, e.g. `dark-peach` for dark theme (or just `dark` for the default dark theme).
      - `examples`: to make your demo *way easier to use*, you can provide some example inputs for the function. These appear below the UI components and can be used to populate the interface. These should be provided as a nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component.         
        
Using these options, we end up with a more complete interface like the one below:

<iframe src="https://hf.space/gradioiframe/course-demos/https://huggingface.co/spaces/course-demos/Rick_and_Morty_QA/+" frameBorder="0" height="800" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

### Sharing Gradio Interfaces
Now that we have a working demo of our machine learning model, let's learn how to easily share a link to our interface.
Interfaces can be easily shared publicly by setting `share=True` in the `launch()` method.
        
```
gr.Interface(classify_image, "image", "label").launch(share=True)
```
        
This generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on!), you don't have to worry about any packaging any dependencies. If you're working out of a Google colab notebook, a share link is always automatically created. It usually looks something like this: **XXXXX.gradio.app**. Although the link is served through a gradio link, we are only a proxy for your local server, and do not store any data sent through the interfaces.

Keep in mind, however, that these links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. If you set `share=False` (the default), only a local link is created, which can be shared by **[port-forwarding](https://www.ssh.com/ssh/tunneling/example)** with specific users.

### Uploading to Hugging Face Spaces

A share link that you can pass around to collegues is cool, but how can you permanently host your demo and have it exist in its own "space" on the internet?

Huggingface provides the infrastructure to permanently host your Gradio model on the internet, for free! Spaces allows you to create and push to a repo, where your gradio interface code will exist in an `app.py` file. See [Huggingface Spaces](https://huggingface.co/spaces) for more information.


## ✏️ Let's apply it!

Using what we just learned, let's create the sketch recognition demo we saw in [section one of this chapter](/course/chapter9/section1). Let's add some customization to our interface and set `share=True` to create a public link we can pass around.

We can load the labels from [class_names.txt](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/class_names.txt) and load the pre-trained pytorch model from [pytorch_model.bin](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/pytorch_model.bin).

```py
from pathlib import Path

import torch
import gradio as gr
from torch import nn


LABELS = Path('class_names.txt').read_text().splitlines()

model = nn.Sequential(
    nn.Conv2d(1, 32, 3, padding='same'),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(32, 64, 3, padding='same'),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(64, 128, 3, padding='same'),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Flatten(),
    nn.Linear(1152, 256),
    nn.ReLU(),
    nn.Linear(256, len(LABELS)),
)
state_dict = torch.load('pytorch_model.bin', map_location='cpu')
model.load_state_dict(state_dict, strict=False)
model.eval()

def predict(im):
    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.

    with torch.no_grad():
        out = model(x)

    probabilities = torch.nn.functional.softmax(out[0], dim=0)

    values, indices = torch.topk(probabilities, 5)

    return {LABELS[i]: v.item() for i, v in zip(indices, values)}


interface = gr.Interface(
    predict, 
    inputs='sketchpad', 
    outputs='label', 
    theme="huggingface", 
    title="Sketch Recognition", 
    description="Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!", 
    article = "<p style='text-align: center'>Sketch Recognition | Demo Model</p>",
    live=True)
interface.launch(share=True)
```

<iframe src="https://hf.space/gradioiframe/dawood/Sketch-Recognition/+" frameBorder="0" height="670" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


We trained and built a sketch recognition demo with the above code; we also added customization to add context to our interface.

In section 5 we will go over more advanced features of gradio.
