# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è WordPiece[[wordpiece-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
]} />

WordPiece - —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π Google –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è BERT. –í–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ –æ–Ω –±—ã–ª –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤–æ –º–Ω–æ–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ BERT, —Ç–∞–∫–∏—Ö –∫–∞–∫ DistilBERT, MobileBERT, Funnel Transformers –∏ MPNET. –û–Ω –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ BPE –≤ –ø–ª–∞–Ω–µ –æ–±—É—á–µ–Ω–∏—è, –Ω–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ-–¥—Ä—É–≥–æ–º—É.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

üí° –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è WordPiece, –≤–ø–ª–æ—Ç—å –¥–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ–≥–æ, –µ—Å–ª–∏ –≤–∞–º –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–±—â–∏–π –æ–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è[[training-algorithm]]

<Tip warning={true}>

‚ö†Ô∏è Google –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–ª –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–≤–æ–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—É—á–µ–Ω–∏—è WordPiece, –ø–æ—ç—Ç–æ–º—É –≤—Å–µ –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–µ - —ç—Ç–æ –Ω–∞—à–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö. –í–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ —Ç–æ—á–Ω—ã –Ω–µ –Ω–∞ 100 %.

</Tip>

–ö–∞–∫ –∏ BPE, WordPiece –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –≤–∫–ª—é—á–∞—é—â–µ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª—å—é, –∏ –Ω–∞—á–∞–ª—å–Ω—ã–π –∞–ª—Ñ–∞–≤–∏—Ç. –ü–æ—Å–∫–æ–ª—å–∫—É –º–æ–¥–µ–ª—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–æ–¥—Å–ª–æ–≤–∞ –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–∞ (–∫–∞–∫ `##` –¥–ª—è BERT), –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏ –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –∫–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞. –¢–∞–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, `"word"` —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```
w ##o ##r ##d
```

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞—á–∞–ª—å–Ω—ã–π –∞–ª—Ñ–∞–≤–∏—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞, –∏ —Å–∏–º–≤–æ–ª—ã, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å WordPiece.

–ó–∞—Ç–µ–º, –∫–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å BPE, WordPiece –∏–∑—É—á–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–ø–æ—Å–æ–±–µ –≤—ã–±–æ—Ä–∞ –ø–∞—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –ø–∞—Ä—É, WordPiece —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –ø–æ —Å–ª–µ–¥—É—é—â–µ–π —Ñ–æ—Ä–º—É–ª–µ:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

–î–µ–ª—è —á–∞—Å—Ç–æ—Ç—É –ø–∞—Ä—ã –Ω–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –∫–∞–∂–¥–æ–π –∏–∑ –µ–µ —á–∞—Å—Ç–µ–π, –∞–ª–≥–æ—Ä–∏—Ç–º –æ—Ç–¥–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Å–ª–∏—è–Ω–∏—é –ø–∞—Ä, –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –∫–æ—Ç–æ—Ä—ã—Ö –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ —Ä–µ–∂–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç `("un", "##able")`, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–∞ –ø–∞—Ä–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ –æ—á–µ–Ω—å —á–∞—Å—Ç–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–≤–µ –ø–∞—Ä—ã `"un"` –∏ `"##able"`, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤ –∏ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫—É—é —á–∞—Å—Ç–æ—Ç—É. –ù–∞–ø—Ä–æ—Ç–∏–≤, —Ç–∞–∫–∞—è –ø–∞—Ä–∞, –∫–∞–∫ `("hu", "##gging")`, –≤–µ—Ä–æ—è—Ç–Ω–æ, –±—É–¥–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ –±—ã—Å—Ç—Ä–µ–µ (–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ —Å–ª–æ–≤–æ  "hugging" —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ), –ø–æ—Å–∫–æ–ª—å–∫—É `"hu"` –∏ `"##gging"` –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ.

–î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç–æ—Ç –∂–µ —Å–ª–æ–≤–∞—Ä—å, –∫–æ—Ç–æ—Ä—ã–π –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ —É—á–µ–±–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

–†–∞–±–∏–µ–Ω–∏–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

–ø–æ—ç—Ç–æ–º—É –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç –∏–º–µ—Ç—å –≤–∏–¥ `["b", "h", "p", "##g", "##n", "##s", "##u"]` (–µ—Å–ª–∏ –º—ã –ø–æ–∫–∞ –∑–∞–±—É–¥–µ–º –æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö). –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞ - `("##u", "##g")` (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 20 —Ä–∞–∑), –Ω–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ `"##u"` –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞, –ø–æ—ç—Ç–æ–º—É –µ–µ –æ—Ü–µ–Ω–∫–∞ –Ω–µ —Å–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è (–æ–Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1/36). –í—Å–µ –ø–∞—Ä—ã —Å `"##u"` —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–º–µ—é—Ç —Ç–∞–∫—É—é –∂–µ –æ—Ü–µ–Ω–∫—É (1/36), –ø–æ—ç—Ç–æ–º—É –ª—É—á—à—É—é –æ—Ü–µ–Ω–∫—É –ø–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞ `("##g", "##s")` - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç `"##u"` - —Å –æ—Ü–µ–Ω–∫–æ–π 1/20, –∏ –ø–µ—Ä–≤—ã–º –≤—ã—É—á–µ–Ω–Ω—ã–º —Å–ª–∏—è–Ω–∏–µ–º –±—É–¥–µ—Ç `("##g", "##s") -> ("##gs")`.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø—Ä–∏ —Å–ª–∏—è–Ω–∏–∏ –º—ã —É–¥–∞–ª—è–µ–º `##` –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ–∫–µ–Ω–∞–º–∏, –ø–æ—ç—Ç–æ–º—É –º—ã –¥–æ–±–∞–≤–ª—è–µ–º `"##gs"` –≤ —Å–ª–æ–≤–∞—Ä—å –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–ª–∏—è–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ö –∫–æ—Ä–ø—É—Å–∞:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

–í —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç `"##u"` –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–æ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–∞—Ä–∞—Ö, –ø–æ—ç—Ç–æ–º—É –≤—Å–µ –æ–Ω–∏ –ø–æ–ª—É—á–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –±–∞–ª–ª. –î–æ–ø—É—Å—Ç–∏–º, –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø–µ—Ä–≤–∞—è –ø–∞—Ä–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è, —Ç–∞–∫ —á—Ç–æ `("h", "##u") -> "hu"`. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –Ω–∞—Å –∫:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

–ó–∞—Ç–µ–º —Å–ª–µ–¥—É—é—â—É—é –ª—É—á—à—É—é –æ—Ü–µ–Ω–∫—É —Ä–∞–∑–¥–µ–ª—è—é—Ç `("hu", "##g")` –∏ `("hu", "##gs")` (1/15, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 1/21 –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–∞—Ä), –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–≤–∞—è –ø–∞—Ä–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –æ—Ü–µ–Ω–∫–æ–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

–∏ –º—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–∞–∫ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è.

<Tip>

‚úèÔ∏è ** –¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –æ—á–µ—Ä–µ–¥—å! ** –ö–∞–∫–∏–º –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ —Å–ª–∏—è–Ω–∏—è?

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏[[tokenization-algorithm]]

Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `"hugs"` the longest subword starting from the beginning that is inside the vocabulary is `"hug"`, so we split there and get `["hug", "##s"]`. We then continue with `"##s"`, which is in the vocabulary, so the tokenization of `"hugs"` is `["hug", "##s"]`.

With BPE, we would have applied the merges learned in order and tokenized this as `["hu", "##gs"]`, so the encoding is different.

As another example, let's see how the word `"bugs"` would be tokenized. `"b"` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `["b", "##ugs"]`. Then `"##u"` is the longest subword starting at the beginning of `"##ugs"` that is in the vocabulary, so we split there and get `["b", "##u, "##gs"]`. Finally, `"##gs"` is in the vocabulary, so this last list is the tokenization of `"bugs"`.

When the tokenization gets to a stage where it's not possible to find a subword in the vocabulary, the whole word is tokenized as unknown -- so, for instance, `"mug"` would be tokenized as `["[UNK]"]`, as would `"bum"` (even if we can begin with `"b"` and `"##u"`, `"##m"` is not the vocabulary, and the resulting tokenization will just be `["[UNK]"]`, not `["b", "##u", "[UNK]"]`). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown.

<Tip>

‚úèÔ∏è **Now your turn!** How will the word `"pugs"` be tokenized?

</Tip>

## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è WordPiece[[implementing-wordpiece]]

Now let's take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won't able to use this on a big corpus.

We will use the same corpus as in the BPE example:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

First, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece tokenizer (like BERT), we will use the `bert-base-cased` tokenizer for the pre-tokenization:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it's the list `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

Next we need to split each word, with all the letters that are not the first prefixed by `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

Now that we are ready for training, let's write a function that computes the score of each pair. We'll need to use this at each step of the training:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

Let's have a look at a part of this dictionary after the initial splits:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

Now, finding the pair with the best score only takes a quick loop:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

So the first merge to learn is `('a', '##b') -> 'ab'`, and we add `'ab'` to the vocabulary:

```python
vocab.append("ab")
```

To continue, we need to apply that merge in our `splits` dictionary. Let's write another function for this:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

And we can have a look at the result of the first merge:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

Now we have everything we need to loop until we have learned all the merges we want. Let's aim for a vocab size of 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

We can then look at the generated vocabulary:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

As we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster.

<Tip>

üí° Using `train_new_from_iterator()` on the same corpus won't result in the exact same vocabulary. This is because the ü§ó Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead.

</Tip>

To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

Let's test it on one word that's in the vocabulary, and another that isn't:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

Now, let's write a function that tokenizes a text:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

We can try it on any text:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

That's it for the WordPiece algorithm! Now let's take a look at Unigram.
