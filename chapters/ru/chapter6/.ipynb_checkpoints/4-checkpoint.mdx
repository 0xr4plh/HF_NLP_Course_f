# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

–ü—Ä–µ–∂–¥–µ —á–µ–º –º—ã –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ–¥—Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –º–æ–¥–µ–ª—è—Ö Transformer (Byte-Pair Encoding [BPE], WordPiece –∏ Unigram), –º—ã —Å–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –∫–æ—Ç–æ—Ä—É—é –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫ —Ç–µ–∫—Å—Ç—É. –í–æ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –æ–±–∑–æ—Ä —ç—Ç–∞–ø–æ–≤ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

–ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø–æ–¥—Ç–æ–∫–µ–Ω—ã (–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é), —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥–≤–∞ —à–∞–≥–∞: _–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é_ –∏ _–ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é_.

## –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

–®–∞–≥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ–∫–æ—Ç–æ—Ä—É—é –æ–±—â—É—é –æ—á–∏—Å—Ç–∫—É, –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏/–∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —É–¥–∞—Ä–µ–Ω–∏–π. –ï—Å–ª–∏ –≤—ã –∑–Ω–∞–∫–æ–º—ã —Å [Unicode normalization](http://www.unicode.org/reports/tr15/) (–Ω–∞–ø—Ä–∏–º–µ—Ä, NFC –∏–ª–∏ NFKC), —ç—Ç–æ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º.

–£ ü§ó Transformers `tokenizer` –µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç `backend_tokenizer`, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –±–∞–∑–æ–≤–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Tokenizers:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

–ê—Ç—Ä–∏–±—É—Ç `normalizer` –æ–±—ä–µ–∫—Ç–∞ `tokenizer` –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥ `normalize_str()`, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("H√©ll√≤ h√¥w are √º?"))
```

```python out
'hello how are u?'
```

–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –≤—ã–±—Ä–∞–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É `bert-base-uncased`, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–∏–ª–∞ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –∏ —É–¥–∞–ª–∏–ª–∞ —É–¥–∞—Ä–µ–Ω–∏—è. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ! ** –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ `bert-base-cased` –∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –µ–º—É —Ç–æ—Ç –∂–µ –ø—Ä–∏–º–µ—Ä. –ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –º–µ–∂–¥—É –≤–µ—Ä—Å–∏–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ cased –∏ uncased?

</Tip>

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

–ö–∞–∫ –º—ã —É–≤–∏–¥–∏–º –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—É—á–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ —Å—ã—Ä–æ–º —Ç–µ–∫—Å—Ç–µ. –°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ —á–∞—Å—Ç–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ —Å–ª–æ–≤–∞. –ò–º–µ–Ω–Ω–æ –≤ —ç—Ç–æ–º –∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è —ç—Ç–∞–ø –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 2] (/course/chapter2), —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤ (word-based tokenizer) –º–æ–∂–µ—Ç –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–±–∏—Ç—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –∑–Ω–∞–∫–∞–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏. –≠—Ç–∏ —Å–ª–æ–≤–∞ —Å—Ç–∞–Ω—É—Ç –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ–¥—Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–º–æ–∂–µ—Ç –≤—ã—É—á–∏—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.

–ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –º—ã –º–æ–∂–µ–º –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–µ—Ç–æ–¥–æ–º `pre_tokenize_str()` –∞—Ç—Ä–∏–±—É—Ç–∞ `pre_tokenizer` –æ–±—ä–µ–∫—Ç–∞ `tokenizer`:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É–∂–µ —Å–ª–µ–¥–∏—Ç –∑–∞ —Å–º–µ—â–µ–Ω–∏—è–º–∏, –∏ –∏–º–µ–Ω–Ω–æ –ø–æ—ç—Ç–æ–º—É –æ–Ω –º–æ–∂–µ—Ç –¥–∞—Ç—å –Ω–∞–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–æ–µ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —Ä–∞–∑–¥–µ–ª–µ. –ó–¥–µ—Å—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –¥–≤–∞ –ø—Ä–æ–±–µ–ª–∞ –∏ –∑–∞–º–µ–Ω—è–µ—Ç –∏—Ö –æ–¥–Ω–∏–º, –Ω–æ —Å–º–µ—â–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ—Ç –º–µ–∂–¥—É `are` –∏ `you`, —á—Ç–æ–±—ã —É—á–µ—Å—Ç—å —ç—Ç–æ.

–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä BERT, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–∞–µ—Ç —á–∞—Å—Ç—å –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é. –î—Ä—É–≥–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —ç—Ç–æ–≥–æ —à–∞–≥–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä GPT-2:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

–æ–Ω —Ç–∞–∫–∂–µ –≤—ã–ø–æ–ª–Ω–∏—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–±–µ–ª—å–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–º–µ–Ω–∏—Ç –∏—Ö —Å–∏–º–≤–æ–ª–æ–º `ƒ†`, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –µ–º—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, –µ—Å–ª–∏ –º—ã –¥–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã:

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),
 ('?', (19, 20))]
```

–¢–∞–∫–∂–µ –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BERT, —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–±–µ–ª.

–í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä T5, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ SentencePiece:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('‚ñÅHello,', (0, 6)), ('‚ñÅhow', (7, 10)), ('‚ñÅare', (11, 14)), ('‚ñÅyou?', (16, 20))]
```

–ö–∞–∫ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä GPT-2, —ç—Ç–æ—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–º–µ–Ω—è–µ—Ç –∏—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º (`_`), –Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä T5 –¥–µ–ª–∞–µ—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º, –∞ –Ω–µ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è. –¢–∞–∫–∂–µ –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –æ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø–µ—Ä–µ–¥ `Hello`) –∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–±–µ–ª –º–µ–∂–¥—É `are` –∏ `you`.

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –Ω–µ–º–Ω–æ–≥–æ –ø–æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å —Ç–µ–º, –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ç–µ–∫—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã, –º–æ–∂–Ω–æ –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ –∏–∑—É—á–µ–Ω–∏—é —Å–∞–º–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –ª–µ–∂–∞—â–∏—Ö –≤ –∏—Ö –æ—Å–Ω–æ–≤–µ. –ú—ã –Ω–∞—á–Ω–µ–º —Å –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–±–∑–æ—Ä–∞ —à–∏—Ä–æ–∫–æ –ø—Ä–∏–º–µ–Ω—è–µ–º–æ–≥–æ SentencePiece; –∑–∞—Ç–µ–º, –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Ç—Ä–µ—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö, –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ –ø–æ–¥—Å–ª–æ–≤–∞–º.

## SentencePiece[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `‚ñÅ`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).

The other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.

## Algorithm overview[[algorithm-overview]]

In the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.


Model | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Training | Starts from a small vocabulary and learns rules to merge tokens |  Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens
Training step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus
Learns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token
Encoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training

Now let's dive into BPE!