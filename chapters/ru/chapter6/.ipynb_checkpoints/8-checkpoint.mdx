# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –±–ª–æ–∫ –∑–∞ –±–ª–æ–∫–æ–º[[building-a-tokenizer-block-by-block]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
]} />

–ö–∞–∫ –º—ã —É–∂–µ –≤–∏–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤:

- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ª—é–±–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –∏–ª–∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode –∏ —Ç. –¥.)
- –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞).
- –ü—Ä–æ–≥–æ–Ω –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤)
- –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤)

–í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–æ—Ç –µ—â–µ –æ–¥–∏–Ω –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —à–∞–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ —Å–º–µ—à–∏–≤–∞—Ç—å –∏ —Å–æ—á–µ—Ç–∞—Ç—å –º–µ–∂–¥—É —Å–æ–±–æ–π. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –Ω—É–ª—è, –∞ –Ω–µ –æ–±—É—á–∞—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ä–æ–≥–æ, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ –≤ [—Ä–∞–∑–¥–µ–ª–µ 2](/course/chapter6/2). –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –≤—ã —Å–º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞—Ç—å –ª—é–±–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ–ª—å–∫–æ —Å–º–æ–∂–µ—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å!

<Youtube id="MR8tZm5ViWU"/>

–¢–æ—á–Ω–µ–µ, –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –≤–æ–∫—Ä—É–≥ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ `Tokenizer`, –∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –≤ –ø–æ–¥–º–æ–¥—É–ª–∏:

- `normalizers` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–∏–ø—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–∞ `Normalizer`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers)).
- `pre_tokenizers` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ `PreTokenizer`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers)).
- `models` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –º–æ–¥–µ–ª–µ–π `Model`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —Ç–∞–∫–∏–µ –∫–∞–∫ `BPE`, `WordPiece` –∏ `Unigram` (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models)).
- `trainers` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã `Trainer`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏; –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers)).
- `post_processors` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ `PostProcessor`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors)).
- `decoders` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –¥–µ–∫–æ–¥–µ—Ä–æ–≤ `Decoder`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders)).

–í–µ—Å—å —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/python/latest/components.html).

## –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–∞[[acquiring-a-corpus]]

–î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ (—á—Ç–æ–±—ã –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω—è–ª–∏—Å—å –±—ã—Å—Ç—Ä–æ). –®–∞–≥–∏ –ø–æ —Å–±–æ—Ä—É –∫–æ—Ä–ø—É—Å–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã —Ç–µ–º, —á—Ç–æ –º—ã –¥–µ–ª–∞–ª–∏ –≤ [–Ω–∞—á–∞–ª–µ —ç—Ç–æ–π –≥–ª–∞–≤—ã](/course/chapter6/2), –Ω–æ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

–§—É–Ω–∫—Ü–∏—è `get_training_corpus()` - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–¥–∞–µ—Ç –±–∞—Ç—á –∏–∑ 1000 —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. 

ü§ó –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö. –í–æ—Ç –∫–∞–∫ –º—ã –º–æ–∂–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤—Å–µ —Ç–µ–∫—Å—Ç—ã/–≤—Ö–æ–¥—ã –∏–∑ WikiText-2, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

–î–∞–ª–µ–µ –º—ã –ø–æ–∫–∞–∂–µ–º –≤–∞–º, –∫–∞–∫ –±–ª–æ–∫ –∑–∞ –±–ª–æ–∫–æ–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã BERT, GPT-2 –∏ XLNet. –≠—Ç–æ –¥–∞—Å—Ç –Ω–∞–º –ø—Ä–∏–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: WordPiece, BPE –∏ Unigram. –ù–∞—á–Ω–µ–º —Å BERT!

## –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ WordPiece —Å –Ω—É–ª—è[[building-a-wordpiece-tokenizer-from-scratch]]

–ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Tokenizers, –º—ã –Ω–∞—á–Ω–µ–º —Å –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ `Tokenizer` –∏ `model`, –∑–∞—Ç–µ–º —É—Å—Ç–∞–Ω–æ–≤–∏–º –¥–ª—è –∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ `normalizer`, `pre_tokenizer`, `post_processor` –∏ `decoder` –Ω—É–∂–Ω—ã–µ –Ω–∞–º –∑–Ω–∞—á–µ–Ω–∏—è.

–î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã —Å–æ–∑–¥–∞–¥–∏–º `Tokenizer` —Å –º–æ–¥–µ–ª—å—é WordPiece:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

–ú—ã –¥–æ–ª–∂–Ω—ã —É–∫–∞–∑–∞—Ç—å `unk_token`, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –∑–Ω–∞–ª–∞, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å, –∫–æ–≥–¥–∞ –æ–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–Ω—å—à–µ –Ω–µ –≤–∏–¥–µ–ª–∞. –î—Ä—É–≥–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –∑–¥–µ—Å—å, –≤–∫–ª—é—á–∞—é—Ç `vocab` –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –µ–≥–æ –∑–∞–¥–∞–≤–∞—Ç—å) –∏ `max_input_chars_per_word`, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (—Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –±—É–¥—É—Ç —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ —á–∞—Å—Ç–∏).

–ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –ø–æ—ç—Ç–æ–º—É –Ω–∞—á–Ω–µ–º —Å –Ω–µ–µ. –ü–æ—Å–∫–æ–ª—å–∫—É BERT —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç `BertNormalizer` —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–ª—è BERT: `lowercase` –∏ `strip_accents`, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –ø–æ—è—Å–Ω–µ–Ω–∏–π; `clean_text` –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ—Ö —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∑–∞–º–µ–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω; –∏ `handle_chinese_chars`, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤. –ß—Ç–æ–±—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `bert-base-uncased`, –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç—Ç–æ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

–û–¥–Ω–∞–∫–æ, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —É –≤–∞—Å –Ω–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç–∞–∫–æ–º—É —É–¥–æ–±–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä—É, —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Tokenizers, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä BERT –≤—Ä—É—á–Ω—É—é. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä `Lowercase` –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä `StripAccents`, –∏ –≤—ã –º–æ–∂–µ—Ç–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

–ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä Unicode `NFD`, –ø–æ—Å–∫–æ–ª—å–∫—É –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä `StripAccents` –Ω–µ —Å–º–æ–∂–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω–µ —É–¥–∞–ª–∏—Ç –∏—Ö.

–ö–∞–∫ –º—ã —É–∂–µ –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `normalize_str()` –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ –æ–Ω –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:

```python
print(tokenizer.normalizer.normalize_str("H√©ll√≤ h√¥w are √º?"))
```

```python out
hello how are u?
```

<Tip>

**–î–∞–ª–µ–µ** –µ—Å–ª–∏ –≤—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç–µ –¥–≤–µ –≤–µ—Ä—Å–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π —Å–∏–º–≤–æ–ª Unicode `u"\u0085"`, —Ç–æ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ –∑–∞–º–µ—Ç–∏—Ç–µ, —á—Ç–æ —ç—Ç–∏ –¥–≤–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ —Å–æ–≤—Å–µ–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã.  
–ß—Ç–æ–±—ã –Ω–µ —É—Å–ª–æ–∂–Ω—è—Ç—å –≤–µ—Ä—Å–∏—é —Å `normalizers.Sequence`, –º—ã –Ω–µ –≤–∫–ª—é—á–∏–ª–∏ –≤ –Ω–µ–µ Regex-–∑–∞–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É–µ—Ç `BertNormalizer`, –∫–æ–≥–¥–∞ –∞—Ä–≥—É–º–µ–Ω—Ç `clean_text` —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ `True`, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. –ù–æ –Ω–µ –≤–æ–ª–Ω—É–π—Ç–µ—Å—å: –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ—á–Ω–æ —Ç–∞–∫—É—é –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É–¥–æ–±–Ω–æ–≥–æ `BertNormalizer`, –¥–æ–±–∞–≤–∏–≤ –¥–≤–∞ `normalizers.Replace` –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤.

</Tip>

–î–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç —ç—Ç–∞–ø –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –û–ø—è—Ç—å –∂–µ, –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–π `BertPreTokenizer`, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

–ò–ª–∏ –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –µ–≥–æ —Å –Ω—É–ª—è:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `Whitespace` —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –±—É–∫–≤–∞–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ —Å–∏–º–≤–æ–ª–æ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è, –ø–æ—ç—Ç–æ–º—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –æ–Ω —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–æ–±–µ–ª—å–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º, —Ç–æ –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —Å–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `WhitespaceSplit`:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Sequence` –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏. –ú—ã —É–∂–µ —É–∫–∞–∑–∞–ª–∏ –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –Ω–∞–º –≤—Å–µ –µ—â–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –µ–µ, –¥–ª—è —á–µ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è `WordPieceTrainer`. –ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å –ø—Ä–∏ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç—Ä–µ–Ω–µ—Ä–∞ –≤ ü§ó Tokenizers, —ç—Ç–æ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–º—É –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Å–æ–±–∏—Ä–∞–µ—Ç–µ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å - –∏–Ω–∞—á–µ –æ–Ω –Ω–µ –¥–æ–±–∞–≤–∏—Ç –∏—Ö –≤ —Å–ª–æ–≤–∞—Ä—å, –ø–æ—Å–∫–æ–ª—å–∫—É –∏—Ö –Ω–µ—Ç –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

–ü–æ–º–∏–º–æ —É–∫–∞–∑–∞–Ω–∏—è `vocab_size` –∏ `special_tokens`, –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å `min_frequency` (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, –∫–æ—Ç–æ—Ä–æ–µ –¥–æ–ª–∂–µ–Ω –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Ç–æ–∫–µ–Ω, —á—Ç–æ–±—ã –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º –≤ —Å–ª–æ–≤–∞—Ä—å) –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å `continuing_subword_prefix` (–µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ –æ—Ç–ª–∏—á–Ω–æ–µ –æ—Ç `##`).

–ß—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Ä–∞–Ω–µ–µ, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

–ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –º—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –ø—É—Å—Ç—ã–º `WordPiece`):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

–í –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö –º—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —Ç–µ–∫—Å—Ç–µ, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `encode()`:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

–ü–æ–ª—É—á–µ–Ω–Ω–æ–µ `encoding` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π `Encoding`, –∫–æ—Ç–æ—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–∞—Ö: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask` –∏ `overflowing`.

–ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞. –ù–∞–º –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω `[CLS]` –≤ –Ω–∞—á–∞–ª–µ –∏ —Ç–æ–∫–µ–Ω `[SEP]` –≤ –∫–æ–Ω—Ü–µ (–∏–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –ø–∞—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π). –î–ª—è —ç—Ç–æ–≥–æ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `TemplateProcessor`, –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —É–∑–Ω–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤ `[CLS]` –∏ `[SEP]` –≤ —Å–ª–æ–≤–∞—Ä–µ:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

–ß—Ç–æ–±—ã –Ω–∞–ø–∏—Å–∞—Ç—å —à–∞–±–ª–æ–Ω –¥–ª—è `TemplateProcessor`, –º—ã –¥–æ–ª–∂–Ω—ã —É–∫–∞–∑–∞—Ç—å, –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –î–ª—è –æ–±–æ–∏—Ö —Å–ª—É—á–∞–µ–≤ –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å; –ø–µ—Ä–≤–æ–µ (–∏–ª–∏ –æ–¥–∏–Ω–æ—á–Ω–æ–µ) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ `$A`, –∞ –≤—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–µ—Å–ª–∏ –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è –ø–∞—Ä–∞) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ `$B`. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö (—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) –º—ã —Ç–∞–∫–∂–µ —É–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∞ (token type ID) –ø–æ—Å–ª–µ –¥–≤–æ–µ—Ç–æ—á–∏—è. 

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —à–∞–±–ª–æ–Ω BERT –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–≥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –∏—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã.

–ö–∞–∫ —Ç–æ–ª—å–∫–æ —ç—Ç–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ, –≤–µ—Ä–Ω–µ–º—Å—è –∫ –Ω–∞—à–µ–º—É –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É –ø—Ä–∏–º–µ—Ä—É:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

–ò –Ω–∞ –ø–∞—Ä–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

–ú—ã –ø–æ—á—Ç–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å –Ω—É–ª—è - –æ—Å—Ç–∞–ª—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - –¥–æ–±–∞–≤–∏—Ç—å –¥–µ–∫–æ–¥–µ—Ä:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏–º –µ–≥–æ –Ω–∞ –Ω–∞—à–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–º `encoding`:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

–û—Ç–ª–∏—á–Ω–æ! –ú—ã –º–æ–∂–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º JSON-—Ñ–∞–π–ª–µ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
tokenizer.save("tokenizer.json")
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç—Ç–æ—Ç —Ñ–∞–π–ª –≤ –æ–±—ä–µ–∫—Ç `Tokenizer` —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `from_file()`:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

–ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ ü§ó Transformers, –º—ã –¥–æ–ª–∂–Ω—ã –æ–±–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ `PreTrainedTokenizerFast`. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏–±–æ –æ–±—â–∏–π –∫–ª–∞—Å—Å, –ª–∏–±–æ, –µ—Å–ª–∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∫–ª–∞—Å—Å (–∑–¥–µ—Å—å `BertTokenizerFast`). –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —ç—Ç–æ—Ç —É—Ä–æ–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç.

–ß—Ç–æ–±—ã –æ–±–µ—Ä–Ω—É—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ `PreTrainedTokenizerFast`, –º—ã –º–æ–∂–µ–º –ª–∏–±–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫–∞–∫ `tokenizer_object`, –ª–∏–±–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∫–∞–∫ `tokenizer_file`. –ì–ª–∞–≤–Ω–æ–µ –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –Ω–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é –∑–∞–¥–∞–≤–∞—Ç—å –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å –Ω–µ –º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ –æ–±—ä–µ–∫—Ç–∞ `tokenizer`, –∫–∞–∫–æ–π —Ç–æ–∫–µ–Ω —è–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–æ–º –º–∞—Å–∫–∏, —Ç–æ–∫–µ–Ω–æ–º `[CLS]` –∏ —Ç. –¥.:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # –í –∫–∞—á–µ—Å—Ç–≤–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

–ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `BertTokenizerFast`), –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —É–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∑–¥–µ—Å—å –∏—Ö –Ω–µ—Ç):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

–ó–∞—Ç–µ–º –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–∞–∫ –∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π ü§ó —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Transformers. –í—ã –º–æ–∂–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –µ–≥–æ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `save_pretrained()` –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ —Ö–∞–± —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `push_to_hub()`.

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏, –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä WordPiece, –¥–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º —Ç–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BPE. –ú—ã –±—É–¥–µ–º –¥–≤–∏–≥–∞—Ç—å—Å—è –Ω–µ–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ, –ø–æ—Å–∫–æ–ª—å–∫—É –≤—ã –∑–Ω–∞–µ—Ç–µ –≤—Å–µ —à–∞–≥–∏, –∏ –ø–æ–¥—á–µ—Ä–∫–Ω–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–ª–∏—á–∏—è.

## –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BPE —Å –Ω—É–ª—è[[building-a-bpe-tokenizer-from-scratch]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä GPT-2. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º BERT, –º—ã –Ω–∞—á–Ω–µ–º —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ `Tokenizer` —Å –º–æ–¥–µ–ª—å—é BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

Also like for BERT, we could initialize this model with a vocabulary if we had one (we would need to pass the `vocab` and `merges` in this case), but since we will train from scratch, we don't need to do that. We also don't need to specify an `unk_token` because GPT-2 uses byte-level BPE, which doesn't require it.

GPT-2 does not use a normalizer, so we skip that step and go directly to the pre-tokenization:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

The option we added to `ByteLevel` here is to not add a space at the beginning of a sentence (which is the default otherwise). We can have a look at the pre-tokenization of an example text like before:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('ƒ†test', (5, 10)), ('ƒ†pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

Next is the model, which needs training. For GPT-2, the only special token is the end-of-text token:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Like with the `WordPieceTrainer`, as well as the `vocab_size` and `special_tokens`, we can specify the `min_frequency` if we want to, or if we have an end-of-word suffix (like `</w>`), we can set it with `end_of_word_suffix`. 

This tokenizer can also be trained on text files:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Let's have a look at the tokenization of a sample text:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'ƒ†test', 'ƒ†this', 'ƒ†to', 'ken', 'izer', '.']
```

We apply the byte-level post-processing for the GPT-2 tokenizer as follows:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

The `trim_offsets = False` option indicates to the post-processor that we should leave the offsets of tokens that begin with 'ƒ†' as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Let's have a look at the result with the text we just encoded, where `'ƒ†test'` is the token at index 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

Finally, we add a byte-level decoder:

```python
tokenizer.decoder = decoders.ByteLevel()
```

and we can double-check it works properly:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

Great! Now that we're done, we can save the tokenizer like before, and wrap it in a `PreTrainedTokenizerFast` or `GPT2TokenizerFast` if we want to use it in ü§ó Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

or:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

As the last example, we'll show you how to build a Unigram tokenizer from scratch.

## Building a Unigram tokenizer from scratch[[building-a-unigram-tokenizer-from-scratch]]

Let's now build an XLNet tokenizer. Like for the previous tokenizers, we start by initializing a `Tokenizer` with a Unigram model:

```python
tokenizer = Tokenizer(models.Unigram())
```

Again, we could initialize this model with a vocabulary if we had one.

For the normalization, XLNet uses a few replacements (which come from SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

This replaces <code>``</code> and <code>''</code> with <code>"</code> and any sequence of two or more spaces with a single space, as well as removing the accents in the texts to tokenize.

The pre-tokenizer to use for any SentencePiece tokenizer is `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

We can have a look at the pre-tokenization of an example text like before:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("‚ñÅLet's", (0, 5)), ('‚ñÅtest', (5, 10)), ('‚ñÅthe', (10, 14)), ('‚ñÅpre-tokenizer!', (14, 29))]
```

Next is the model, which needs training. XLNet has quite a few special tokens:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

A very important argument not to forget for the `UnigramTrainer` is the `unk_token`. We can also pass along other arguments specific to the Unigram algorithm, such as the `shrinking_factor` for each step where we remove tokens (defaults to 0.75) or the `max_piece_length` to specify the maximum length of a given token (defaults to 16).

This tokenizer can also be trained on text files:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Let's have a look at the tokenization of a sample text:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['‚ñÅLet', "'", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.']
```

A peculiarity of XLNet is that it puts the `<cls>` token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens). It's padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the `<cls>` and `<sep>` tokens:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

The template looks like this:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

And we can test it works by encoding a pair of sentences:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['‚ñÅLet', "'", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.', '.', '.', '<sep>', '‚ñÅ', 'on', '‚ñÅ', 'a', '‚ñÅpair', 
  '‚ñÅof', '‚ñÅsentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

Finally, we add a `Metaspace` decoder:

```python
tokenizer.decoder = decoders.Metaspace()
```

and we're done with this tokenizer! We can save the tokenizer like before, and wrap it in a `PreTrainedTokenizerFast` or `XLNetTokenizerFast` if we want to use it in ü§ó Transformers. One thing to note when using `PreTrainedTokenizerFast` is that on top of the special tokens, we need to tell the ü§ó Transformers library to pad on the left:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

Or alternatively:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

Now that you have seen how the various building blocks are used to build existing tokenizers, you should be able to write any tokenizer you want with the ü§ó Tokenizers library and be able to use it in ü§ó Transformers.
