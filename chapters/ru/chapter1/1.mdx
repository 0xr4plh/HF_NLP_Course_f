# –í–≤–µ–¥–µ–Ω–∏–µ

## –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ ü§ó –∫—É—Ä—Å!

<Youtube id="00GKzGyWFEs" />

–í —ç—Ç–æ–º –∫—É—Ä—Å–µ –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å –æ—Å–Ω–æ–≤–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫ –æ—Ç [Hugging Face](https://huggingface.co/). –≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑: –º–æ–¥–µ–ª–µ–π ([ü§ó Transformers](https://github.com/huggingface/transformers)), –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ([ü§ó Datasets](https://github.com/huggingface/datasets)), –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–∏–ª–∏–æ—Ç–µ–∫ ([ü§ó Accelerate](https://github.com/huggingface/accelerate), [ü§ó Tokenizers](https://github.com/huggingface/tokenizers)), –∞ —Ç–∞–∫–∂–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è [Hugging Face Hub](https://huggingface.co/models). –≠—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –±–µ—Å–ø–ª–∞—Ç–Ω–æ!

## –ß–µ–≥–æ –æ–∂–∏–¥–∞—Ç—å –æ—Ç –∫—É—Ä—Å–∞?

–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫—É—Ä—Å–∞:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- –ì–ª–∞–≤—ã 1-4 —Å–æ–¥–µ—Ä–∂–∞—Ç –≤ —Å–µ–±–µ –≤–≤–µ–¥–µ–Ω–∏–µ –≤ –≥–ª–∞–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Transformers. –ö –∫–æ–Ω—Ü—É —ç—Ç–æ–π —á–∞—Å—Ç–∏ –∫—É—Ä—Å–∞ –≤—ã –±—É–¥–µ—Ç–µ –∑–Ω–∞–∫–æ–º—ã —Å —Ç–µ–º, –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, –∫–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è [Hugging Face Hub](https://huggingface.co/models), –∫–∞–∫ –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Hugging Face Hub!
- –ì–ª–∞–≤—ã 5-8 –Ω–∞—É—á–∞—Ç –≤–∞—Å –æ—Å–Ω–æ–≤–∞–º —Ä–∞–∑–¥–µ–ª–æ–≤ ü§ó Datasets –∏ ü§ó Tokenizers (–¥–∞—Ç–∞—Å–µ—Ç—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã), —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –ø–æ–≥—Ä—É–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ö –∫–æ–Ω—Ü—É —ç—Ç–æ–π —á–∞—Å—Ç–∏ –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å —Ä–µ—à–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ ¬†NLP —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ!
- –ì–ª–∞–≤—ã 9-12 –≤—ã—Ö–æ–¥—è—Ç –∑–∞ —Ä–∞–º–∫–∏ NLP, –≤ –Ω–∏—Ö –æ–ø–∏—Å–∞–Ω–æ, –∫–∞–∫ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏. –¢–∞–∫–∂–µ –≤—ã —É–∑–Ω–∞–µ—Ç–µ, –∫–∞–∫ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ü–æ—Å–ª–µ –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–π —á–∞—Å—Ç–∏ –≤—ã –±—É–¥–µ—Ç–µ –≤ —Å–∏–ª–∞—Ö –ø—Ä–∏–º–µ–Ω–∏—Ç—å ü§ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∫ (–ø–æ—á—Ç–∏) –ª—é–±–æ–π –∑–∞–¥–∞—á–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è!

–≠—Ç–æ—Ç –∫—É—Ä—Å: 

* –¢—Ä–µ–±—É–µ—Ç—Å—è —Ö–æ—Ä–æ—à–µ–≥–æ –∑–Ω–∞–Ω–∏—è Python
* –ë—É–¥–µ—Ç –ª—É—á—à–µ —É—Å–≤–æ–µ–Ω –ø–æ—Å–ª–µ –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å –∫—É—Ä—Å–æ–º –ø–æ –≥–ª—É–±–æ–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –Ω–∞–ø—Ä–∏–º–µ—Ä: [fast.ai's](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) –∏–ª–∏ –æ–¥–Ω–æ–π –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º, –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö [DeepLearning.AI](https://www.deeplearning.ai/)
* –ù–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –±–∏–±–ª–∏–æ—Ç–µ–∫: [PyTorch](https://pytorch.org/) –∏–ª–∏ [TensorFlow](https://www.tensorflow.org/), –æ–¥–Ω–∞–∫–æ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å –Ω–∏–º–∏ –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –∫—É—Ä—Å–∞ –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –æ—Ç DeepLearning.AI: [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫—Ä—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π NLP: –æ—Ç –Ω–∞–∏–≤–Ω–æ–≥–æ –ë–∞–π–µ—Å–∞ –¥–æ LSTM-—Å–µ—Ç–µ–π!


## –ö—Ç–æ –º—ã?

–û–± –∞–≤—Ç–æ—Ä–∞—Ö:

**Matthew Carrigan** is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we're going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless.

**Lysandre Debut** is a Machine Learning Engineer at Hugging Face and has been working on the ü§ó Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API.

**Sylvain Gugger** is a Research Engineer at Hugging Face and one of the core maintainers of the ü§ó Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources.

**Merve Noyan** is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone.

**Lucile Saulnier** is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience.

**Lewis Tunstall**  is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of an upcoming [O‚ÄôReilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/).

**Leandro von Werra**  is a machine learning engineer in the open-source team at Hugging Face and also a co-author of the an upcoming [O‚ÄôReilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/). He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack..

Are you ready to roll? In this chapter, you will learn:
* How to use the `pipeline()` function to solve NLP tasks such as text generation and classification
* About the Transformer architecture
* How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases
