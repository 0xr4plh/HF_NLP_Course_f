

<FrameworkSwitchCourse {fw} />

# پردازش داده 
# Processing the data

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}

در این بخش در ادامه مثال [فصل قبل](/course/chapter2)، نحوه آموزش یک مدل ترتیبی در یک بَتچ توسط pytorch را شرح می‌دهیم:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}

در این بخش در ادامه مثال [فصل قبل](/course/chapter2)، نحوه آموزش یک مدل ترتیبی در یک بَتچ توسط pytorch را شرح می‌دهیم:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

{/if}

البته آموزش با استفاده از دو جمله به نتایج چشمگیری منتهی نخواهد شد. برای بدست آوردن نتایج بهتر نیاز به آماده سازی مجموعه‌داده بزرگتر خواهید داشت.

در این بخش ما از مجموعه‌داده MRPC (Microsoft Research Paraphrase Corpus) که در یک [مقاله](https://www.aclweb.org/anthology/I05-5002.pdf)، نوشته‌ی William B. Dolan و Chris Brockett.، معرفی شده به عنوان یک مثال استفاده خواهیم کرد. این داده شامل ۵،۸۰۱ جفت جمله و یک برچسب می‌باشد که برچسب نشاندهنده متناظر بودن جملات (به عنوان مثال، آیا دو جمله معنی یکسانی دارند یا خیر) می‌باشد. علت انتخاب این مجموعه‌داده این است که مجموعه‌داده کوچکیست و تجربه آموزش دادن روی آن آسان است.

### بارگذاری یک داده از هاب

### Loading a dataset from the Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

هاب تنها شامل مدلها نمی‌باشد؛ بلکه شامل مجموعه‌داده‌های متعدد در بسیاری زبان‌های مختلف می‌باشد. شما می‌توانید مجموعه‌داده‌ها را در این [لینک](https://huggingface.co/datasets) جستجو کنید و پیشنهاد می‌کنیم پس از اتمام این بخش یک مجموعه‌داده جدید را دریافت و پردازش کنید (بخش مستندات عمومی را در [اینجا](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub) مشاهده کنید). اما اجازه بدهید اکنون روی داده MRPC تمرکز کنیم! این داده یکی از ۱۰ داده [GLUE benchmark](https://gluebenchmark.com/) است که یک محک تهیه شده در محیط دانشگاهی جهت اندازه گیری کارکرد مدلهای یادگیری ماشینی در ۱۰ مسئله دسته‌بندی متن مختلف می‌باشد.

کتابخانه مجموعه‌داده 🤗 یک دستور بسیار ساده جهت دانلود و ذخیره سازی یک مجموعه‌داده در هاب ارائه می‌کند. ما می‌توانیم مجموعه‌داده MRPC را به روش زیر دانلود کنیم:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

همانطور که می بینید یک شیء "DatasetDict" بدست می‌آوریم که شامل مجموعه training، مجموعه validation، و مجموعه test می‌باشد. هریک از اینها شامل چندین سطون (`label`، `sentence2`، `sentence1`، و `idx`) و تعداد متغیری ردیف داده که عناصر هر مجموعه را تشکیل می‌دهند می‌باشد. (بنابراین، ۳،۶۶۸ جفت جمله در مجموعه training وجود دارد، ۴۰۸ تا در مجموعه validation، و ۱،۷۲۵ تا در مجموعه test).

 این دستور مجموعه‌داده را دانلود و به صورت پیش فرض در زیرشاخه‌ی *~/.cache/huggingface/dataset* ذخیره میکند. از فصل ۲ به یاد داشته باشید که می‌توانید زیرشاخه‌ی ذخیره‌سازیتان را با تنظیم متغیر محیطی `HF_HOME` به دلخواه تغییر دهید.

ما می‌توانیم به هر جفت از جملات در شئ `raw_datasets` با استفاده از اندیس گذاری, مانند یک dictionary دسترسی پیدا کنیم: 

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

می‌بینم که برچسب‌ها از پیش اعداد صحیح هستند، بنابراین مجبور نیستیم هیچ پیش‌پردازشی روی آنها انجام دهیم. برای اینکه بدانیم کدام مقدارِ عددیِ صحیح به کدام برچسب مربوط می‌شود، می‌توانیم ویژگی‌های ‌`raw_train_dataset`‌مان را بررسی کنیم. این کار نوع هر سطون را به ما خواهد گفت.

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

در پشت صحنه، `برچسب` از نوع `برچسبِ کلاس` می‌باشد، و نگاشت اعداد صحیح به نام برچسب در پوشه‌ي *names* ذخیره شده است. `0` مربوط به `not_equivalent`، و `1` مربوط به `equivalent` می‌باشد،.

<Tip>
✏️ **امتحان کنید!** عنصر شماره ۱۵ از داده training و عنصر شماره ۸۷ از داده validation را مشاهده کنید. برچسبهای آنها چیست؟
</Tip>

### پیش‌پردازشِ یک دیتاسِت

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

به منظور پیش‌پردازش دیتاسِت، لازم است متن را به اعدادی قابل پردازش برای مدل تبدیل کنیم. همانطور که در[فصل قبل](/course/chapter2) مشاهده کردید، این کار با استفاده از یک توکنایزر انجام می‌شود. ما می‌توانیم یک یا چند جمله را به توکنایزر بدهیم، در نتیجه می‌توانیم مستقیما تمام جملات اول و دوم هر جفت جمله را به صورت زیر توکنایز کنیم:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

با این حال، نمی‌توانیم دو جمله را به مدل ارسال کنیم تا پیش‌بینی کند متناظر هستند یا خیر. ما نیاز داریم با دو رشته به صورت یک جفت برخورد کنیم، و پیش‌پردازش مناسب را به آن اعمال کنیم. خوشبختانه، توکنایزر می‌تواند یک جفت رشته را دریافت کند و آنرا به گونه‌ای که مدل BERT ما انتظار دارد آماده سازی کند:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

در [فصل ۲](/course/chapter2) در مورد کلیدهای `input_ids` و `attention_mask` بحث کردیم، اما از گفتگو در مورد `token_type_ids` اجتناب کردیم. در این مثال این همان چیزی است که به مدل می‌گوید کدام بخش از ورودی، جمله اول و کدام بخش جمله دوم است.

<Tip>

✏️ **امتحان کنید!** عنصر شماره ۱۵ داده را بردارید و دو جمله را به صورت جداگانه و جفت توکنایز کنید. تفاوت دو نتیجه چیست؟

</Tip>

اگر آیدی‌های داخل `input_ids` را به کلمات رمزگشایی کنیم:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

خواهیم داشت:

we will get:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

بنابراین می‌بینیم که مدل انتظار دارد وقتی که دو جمله داریم ورودی‌ها به صورت `[CLS] sentence1 [SEP] sentence2 [SEP]` باشند.

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

همانطور که می‌بینید، بخشهایی از ورودی که مربوط به `[CLS] sentence1 [SEP]` هستند آیدی نشاندهنده نوع توکِن آنها `0` و بخشهایی که مربوط به `sentence2 [SEP]` هستند آیدی نشاندهنده نوع توکِن‌شان `1` می‌باشد.

توجه داشته باشید که اگر چکپوینت متفاوتی را انتخاب کنید، در ورودی‌ها لزوما `token_type_ids` نخواهید داشت (به عنوان مثال، اگر از یک DistilBERT استفاده کنید آنها بازگردانده نخواهند شد). آنها فقط زمانی بازگردانده می‌شوند که مدل می‌داند با آنها چکار کند، به این خاطر که آنها را در زمان پیش‌تعلیم دیده است.

در اینجا، مدل BERT با آیدی‌هایی که نشاندهنده نوعِ توکن هستند پیش‌تعلیم شده، و علاوه بر هدف مدل سازی زبانِ ماسکی که در [فصل ۱](/course/chapter1) در مورد آن صحبت کردیم وظیفه‌ی دیگری تحت عنوان _پیش‌بینی جمله‌ی بعدی_ برعهده دارد. هدف از این وظیفه مدل کردن رابطه بین جفت جمله‌ها می‌باشد.

 در روش پیش‌بینی جمله بعدی، جفت جملاتی (با کلماتی که به طور تصادفی پنهان شده‌اند) به مدل داده می‌شوند و از مدل خواسته می‌شود پیش‌بینی کند که آیا جمله دوم در ادامه‌ جمله‌ اول قرار دارد یا خیر. برای خارج کردن مسئله از حالت بدیهی، در نیمی از حالتها دوجمله در متن اصلی به دنبال هم آمده‌، و در نیمی دیگر از دو متن متفاوت می‌آیند.

در مجموع، نیازی نیست نگران وجود یا عدم وجود `token_type_ids` در ورودی‌های توکنایز شده خود باشید: مادامی که از چکپوینت یکسان برای توکنایزر و مدل استفاده کنید، همه چیز خوب پیش خواهد رفت چرا که توکنایزر می‌داند چه چیزی برای مدل فراهم کند.

اکنون که مشاهده کردیم چگونه توکنایزر ما می‌تواند با یک جفت جمله برخورد کند، می‌توانیم آنرا برای توکنایز کردن کل دیتاسِتمان به کار ببریم: مانند [فصل قبل](/course/chapter2)، ما می‌توانیم نوکنایزر را با لیستی از جفت جمله‌ها، با دادن لیست جملات اول و سپس لیست جملات دوم، تغذیه کنیم. این روش همچنین با گزینه‌های padding و truncation که در [فصل ۲](/course/chapter2) مشاهده کردیم سازگاری دارد. بنابراین، یک روش برای پیش‌پردازشِ دیتاسِت training اینگونه می‌باشد:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```
این روش به خوبی کار می‌کند، اما مشکلش این است که یک dictionary برمیگرداند (از کلیدهای ما شامل، `input_ids`, `attention_mask`, و `token_type_ids`, و مقادیر آنها که مجموعه‌ای از مجموعه‌ها هستند). همچنین این روش فقط زمانی کار می‌کند که حافظه موقت کافی جهت ذخیره‌سازی کل دیتاسِت در حین توکنایز کردن داشته باشید (در حالی که دیتاسِت‌های موجود در پایگاه دیتاسِت 🤗 فایل‌هایی از نوع [Apache Arrow](https://arrow.apache.org/) هستند که روی دیسک ذخیره شده‌اند، بنابراین شما فقط نمونه‌هایی را که جهت ذخیره در حافظه درخواست کرده‌اید نگه می‌دارید).

به منظور نگه داشتن داده به صورت یک دیتاسِت، از تابع [`()Dataset.map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) استفاده میکنیم. چنانچه به پیش‌پردازش‌های بیشتری علاوه‌بر توکنایز کردن نیاز داشته باشیم این روش انعطاف‌پذری لازم را به ما می‌دهد. تابع `()map` با اعمال کردن یک عملیات روی هر عنصر دیتاسِت عمل می‌کند، بنابراین اجازه دهید تابعی تعریف کنیم که ورودی‌ها را توکنایز کند:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

این تابع یک دیکشنری (مثل اقلام داخل دیتاسِت) دریافت میکند و دیکشنری دیگری با کلیدهای `input_ids`، `attention_mask`، و `token_type_ids` بازمی‌گرداند. توجه داشته باشید از آنجایی که توکنایزر روی لیستهایی از جفت جمله‌ها کار می‌کند، همانطور که قبلا مشاهده کردیم، این تابع نیز درصورتی که دیکشنری `example` شامل چندین نمونه (هر کلید به عنوان مجموعه‌ای از جملات) باشد کار می‌کند. این به ما این امکان را خواهد داد که از گزینه `batched=True` در فراخوانی تابع `()map` استفاده کنیم که توکنایزر را به میزان زیادی سریعتر خواهد کرد. این توکنایزر با توکنایزری در کتابخانه [🤗 Tokenizers](https://github.com/huggingface/tokenizers) که به زبان برنامه‌نویسی Rust نوشته شده پشتیبانی می‌شود. این توکنایزر می‌تواند بسیار سریع باشد، اما فقط به شرطی که ورودی‌های زیادی را به صورت یکجا به آن بدهیم.

توجه داشته باشید که ما مبحث `padding` را در حال حاضر کنار گذاشته‌ایم. این به این خاطر است که انجام `padding` روی همه نمونه‌ها برای بیشترین طول به صرفه نیست: بهتر است که `padding` زمانی که در حال ساختن بَتچ هستیم روی نمونه‌ها انجام دهیم، چرا که آنوقت فقط نیاز داریم که `padding` را روی همان بَتچ انجام دهیم، و نه بیشترین طول در سرتاسر دیتاسِت. این روش زمانی که ورودی‌ها دارای طول‌های بسیار متغیری هستند وقت و انرژی زیادی را صرفه جویی خواهد کرد. 

در اینجا نشان می‌دهیم چگونه تابع توکنایزیشن را روی کل دیتاسِت به یکباره اعمال می‌کنیم. ما از `batched=True` در فراخوانی تابع `map` استفاده می‌کنیم بنابراین تابع روی چندین عنصر از دیتاسِت ما به یکباره عمل می‌کند نه روی هر عنصر به صورت جداگانه. این اجازه می‌دهد که پیش‌پردازش سریعتر انجام گیرد:

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```
روشی که کتابخانه دیتاسِت 🤗 این پیش‌پردازش را اعمال می‌کند، با افزودن -فیلدهای- جدید به دیتاسِت‌ها، یکی به اِزای هر کلید در -دیکشنری- که توسط تابع پیش‌پردازش بازگردانده می‌شوند، می‌باشد:،

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

شما حتی می‌توانید زمانی که تابع پیش‌پردازش خود را اعمال می‌کنید، با ارسال آرگومان `num_proc` در تابع `()map` از پردازش موازی استفاده کنید. ما آن را در اینجا استفاده نکردیم به این خاطر که کتابخانه توکنایزر 🤗 از پیش از چندین رشته پردازنده برای توکنایز کردن سریعتر نمونه‌های ما استفاده می‌کند، اما در صورتی که از یک توکنایزر سریع که با این کتابخانه پشتیبانی میشود استفاده نمی‌کنید، این روش میتواند پیش‌پردازش شما را سریعتر کند.


تابع `tokenize_function` ما یک دیکشنری شامل کلیدهای `input_ids`، `attention_mask`، و `token_type_ids`، برمیگرداند به گونه‌ای که این سه فیلد به همه بخش‌های دیتاسِت افزوده گردند. توجه داشته باشید که اگر تابع پیش‌پردازش ما برای یک کلید موجود در دیتاسِت که تابع `()map` به آن اعمال شده یک مقدار جدید بازمی‌گردانید همچنین می‌توانستیم فیلدهای موجود را تغییر دهیم.

آخرین چیزی که نیاز داریم انجام دهیم این است که هنگامی که عناصر را باهم بَتچ می‌کنیم، همه نمونه‌ها را به اندازه طول بلندترین عنصر هم‌طول کنیم - تکنیکی که ما به آن *هم‌طول‌سازی پویا* می‌گوییم.


### هم‌طول‌سازی پویا

### Dynamic padding

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}

تابعی که مسئول کنارهم گذاشتن نمونه‌ها در یک بَتچ می‌باشد *collate function* خوانده می‌شود. این تابع آرگومانی است که شما می‌توانید هنگام ساختن یک `DataLoader` به داخل آن ارسال کنید، که در حالت پیش فرض تابعی است که نمونه‌های شما را به ماتریس PyTorch تبدیل کرده و آنها را بهم الحاق می‌کند (اگر عناصر شما لیست، تاپل، یا دیکشنری باشند این کار به صورت بازگشتی انجام می‌گیرد). از آنجایی که ورودی‌های ما همه به یک اندازه نیستند این کار برای ما امکان پذیر نیست. ما پروسه هم‌طول‌سازی را عمدا به تعویق انداختیم، تا فقط در زمان نیاز آنرا روی هر بَتچ اجرا کنیم و از داشتن ورودی‌های بیش از اندازه طولانی با تعداد زیادی هم‌طول‌سازی پیشگیری کنیم. این روش پروسه آموزش را تا اندازه‌ای سرعت می‌بخشد، اما توجه داشته باشید که اگر شما در حال آموزش روی TPU هستید می‌تواند مشکل ساز باشد - TPU ها اشکال معین را ترجیح می‌دهند، حتی اگر نیاز به هم‌طول‌سازی اضافه داشته باشد.

{:else}

تابعی که مسئول کنارهم گذاشتن نمونه‌ها در یک بَتچ می‌باشد *collate function* خوانده می‌شود. collator پیش فرض تابعی است که فقط نمونه‌های شما را به tf.Tensor تبدیل کرده و آنها را بهم الحاق می‌کند (اگر عناصر شما لیست، تاپل، یا دیکشنری باشند این کار به صورت بازگشتی انجام می‌گیرد). از آنجایی که ورودی‌ها ما همه به یک اندازه نیستند این کار برای ما امکان پذیر نیست. ما پروسه هم‌طول‌سازی را عمدا به تعویق انداختیم، تا فقط در زمان نیاز آنرا روی هر بَتچ اجرا کنیم و از داشتن ورودی‌های بیش از اندازه طولانی با تعداد زیادی هم‌طول‌سازی پیشگیری کنیم. این روش پروسه آموزش را تا اندازه‌ای سرعت می‌بخشد، اما توجه داشته باشید که اگر شما در حال آموزش روی TPU هستید می‌تواند مشکل ساز باشد - TPU اشکال معین را ترجیح می‌دهند، حتی اگر نیاز به هم‌طول‌سازی اضافه داشته باشد.

{/if}

برای انجام اینکار در عمل، ما باید یک تابع collate تعریف کنیم که میزان درستی از هم‌طول‌سازی را به آیتم‌های دیتاسِت‌هایی که ما می‌خواهیم باهم بَتچ کنیم اعمال کند. خوشبختانه، کتابخانه ترنسفورمرهای 🤗 چنین تابعی را تحت عنوان `DataCollatorWithPadding` در اختیار ما قرار میدهد. این تابع به محض اینکه آنرا تعریف (یعنی تعیین کنیم چه توکنی برای هم‌طول‌سازی استفاده کند، و اینکه مدل انتظار هم‌طول‌سازی از سمت چپ ورودی‌ها را داشته باشد یا از سمت راست آنها) یک توکنایزر را برداشته و هر کاری را که لازم دارید انجام می‌دهد:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

اجازه دهید چند نمونه از مجموعه training مان، که می‌خواهیم باهم بَتچ کنیم، برداریم تا این ابزار جدید را امتحان کنیم. در اینجا سطون‌های `idx`، `sentence1`، و `sentence2` را حذف می‌کنیم چرا که احتیاج نخواهند شد و شامل رشته‌های متنی می‌شوند (که ما نمی‌توانیم تنسورهایی از رشته‌های متنی ایجاد کنیم) و سپس نگاهی می‌اندازیم به طول هر ورودی در هر بَتچ:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

تعجبی ندارد که نمونه‌هایی با طول‌های متغییر، از ۳۲ تا ۶۷ بدست می‌آوریم. هم‌طول‌سازی پویا به این معنی است که نمونه‌های موجود در این بَتچ باید همگی با طول ۶۷، که بزرگترین طول داخل بَتچ می‌باشد، هم‌طول شده باشند. بدون هم‌طول‌سازی پویا، همه نمونه‌ها در کل دیتاسِت باید به اندازه بزرگترین طول یا بزرگترین طولِ قابل پذیرش برای مدل، هم‌طول شوند. اجازه دهید بررسی کنیم آیا `data_collator` ما بَتچ را به درستی هم‌طول می‌کند:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

به نظر خوب می‌آید! اکنون که از متن خالص به بَتچ‌هایی رسیده‌ایم که مدل مان می‌تواند با آنها کار کند، آماده هستیم برای انجام کوک‌ کردن مدل:

{/if}

<Tip>

✏️ **امتحان کنید!** پروسه پیش‌پردازش را روی دیتاسِت GLUE SST-2 باز تکرار کنید. از آنجایی که این مجموعه به جای جفت شامل تک جمله‌ها می‌باشد این کار یک مقدار متفاوت است، اما بقیه کارهایی که انجام داده‌ایم باید یکسان به نظر برسند. برای یک چالش مشکل‌تر، سعی کنید تابع پیش‌پردازشی بنویسید که برای همه مسئله‌های GLUE کار کند.

</Tip>

{#if fw === 'tf'}

توجه داشته باشید که ما مجموعه‌‌داده مان و یک collator داده در اختیار داریم، حال نیاز داریم که آنها را کنار هم قرار دهیم. ما می‌توانستیم بَتچ‌ها را دستی لود کرده و آنها را collate کنیم، اما این روش کار زیادی می‌برد و احتمالا خیلی هم بهینه نخواهد بود. در عوض، یک روش ساده وجود دارد که راه حل بهینه‌ای برای این مسئله ارائه میکند: `()to_tf_dataset`. این روش یک `tf.data.Dataset` را دور دیتاسِت‌تان می‌پیچد، با یک تابع collation دلخواه. `tf.data.Dataset` یک فرمت بومی تنسورفلو است که کِراس می‌تواند برای `()model.fit` استفاده کند، در تنیجه همین یک تابع می‌تواند یک دیتاسِت 🤗 را  به سرعت به فرمت آماده برای آموزش تبدیل کند. اجازه دهید آنرا در عمل با دیتاسِت‌مان مشاهده کنیم!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

این هم از این! حالا می‌توانیم این دیتاسِت‌ها را به درس بعدی ببریم، جایی که آموزش پس از همه سختی‌های  پیش‌پردازش به طرز خوشایندی سرراست خواهد بود.

{/if}
