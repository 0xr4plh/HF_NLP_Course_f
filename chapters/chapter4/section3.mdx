<script>
	import Question from "../../Question.svelte";
	import Tip from "../../Tip.svelte";
	import Youtube from "../../Youtube.svelte";
	
	export let fw: "pt" | "tf";
</script>

The easiest way to share a pretrained model is by using the Hugging Face Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below.

<Youtube id="rkCly_cbMBk"/>

We encourage all users that train models to contribute by sharing them with the community ‚Äî sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done!

There are three ways to go about creating new model repositories:

- Using the `push_to_hub` API
- Using the `transformers` CLI
- Using the web interface

Once you've created a repository, you can upload files to it via `git` and `git-lfs`. We'll walk you through creating model repositories and uploading files to them in the following sections.


## Using the `push_to_hub` API through `huggingface_hub`

<Youtube id="A5IWIxsHLUw"/>

The simplest way to upload files to the Hub is by leveraging the `push_to_hub` method.

Before going further, you'll need to generate an authentication token so that the `huggingface_hub` API knows who you are and what namespaces you have write access to. Make sure you are in an environment where you have `transformers` installed (see [Setup](/course/chapter0)), and run the CLI `login` command (make sure to prepend these commands with the `!` character if running in Google Colab):

```bash
transformers-cli login
```

Or equivalently if `huggingface_hub` is installed in your environment:

```bash
huggingface-cli login
```

You should be prompted for your username and password, which are the same ones you use to log in to the Hub. If you do not have a Hub profile yet, you should create one [here](https://huggingface.co/join).

Great! You now have your authentication token stored in your cache folder. Let's create some repositories!

Accessing the Model Hub through `transformers` is done via the `push_to_hub` method, which is available on models, tokenizers, and configuration objects. This method takes care of both the repository creation and pushing the model and tokenizer files directly to the repository. No manual handling is required, unlike with the two methods we'll see below.

{#if fw === 'pt'}
Additionally, if you have followed the PyTorch tutorials until now and have played around with the `Trainer` API to train a model, you can directly push the model and tokenizer together by executing `trainer.push_to_hub("<model_name>")`. In addition to creating the new repository and pushing the model and tokenizer files, it will also generate a model card with all the relevant metadata, reporting the hyperparameters used and the evaluation results! Here is an example of the content you might find in a such a model card:

<p align="center">
  <img src="/course/static/chapter4/model_card.png" alt="An example of an auto-generated model card." width="100%"/>
</p>
{/if}

To get an idea of how it works, let's first initialize a model and a tokenizer:

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{/if}

You're free to do whatever you want with these ‚Äî add tokens to the tokenizer, train the model, fine-tune it. Once you're happy with the resulting model, weights, and tokenizer, you can leverage the `push_to_hub` method directly available on the `model` object:

```py
model.push_to_hub("dummy-model")
```

This will create the new repository `dummy-model` in your profile, and populate it with your model files.
Do the same with the tokenizer, so that all the files are now available in this repository:

```py
tokenizer.push_to_hub("dummy-model")
```

If you belong to an organization, simply specify the `organization` argument to upload to that organization's namespace:

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface")
```

If you wish to use a specific Hugging Face token, you're free to specify it to the `push_to_hub` method as well:

```py
tokenizer.push_to_hub(
    "dummy-model", organization="huggingface", use_auth_token="<TOKEN>"
)
```

Now head to the Model Hub to find your newly uploaded model: *https://huggingface.co/user-or-organization/dummy-model*.

Click on the "Files and versions" tab, and you should see the files visible in the following screenshot:

{#if fw === 'pt'}
<p align="center">
<img src="/course/static/chapter4/push_to_hub_dummy_model.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</p>
{:else}
<p align="center">
<img src="/course/static/chapter4/push_to_hub_dummy_model_tf.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</p>
{/if}

<Tip>

‚úèÔ∏è **Try it out!** Take the model and tokenizer associated with the `bert-base-cased` checkpoint and upload them to a repo in your namespace using the `push_to_hub` method. Double-check that the repo appears properly on your page before deleting it.

</Tip>

As you've seen, the `push_to_hub` method accepts several arguments, making it possible to upload to a specific repository or organization namespace, or to use a different API token. We recommend you take a look at the method specification available directly in the [ü§ó Transformers documentation](https://huggingface.co/transformers/model_sharing.html) to get an idea of what is possible.

The `push_to_hub` method is backed by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python package, which offers a direct API to the Hugging Face Hub. It's integrated within ü§ó Transformers and several other machine learning libraries, like [`allenlp`](https://github.com/allenai/allennlp). Although we focus on the ü§ó Transformers integration in this chapter, integrating it into your own code or library is simple.

Jump to the last section to see how to upload files to your newly created repository!

## Using the `transformers` CLI

The `transformers` package offers a very simple CLI for common tasks like getting information about your environment, adding a new model architecture to the library, and converting a model from other implementations to `transformers`. It also offers a simple command to create a new repository in your namespace or a given organization's namespace on the Hub.

The CLI lets you create a new repository, but does not populate it with files. In this section you'll see how you can use `git` to upload files from the command line.

Similarly to using the `huggingface_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab):

```bash
transformers-cli login
```

Or equivalently if `huggingface_hub` is installed in your environment:

```bash
huggingface-cli login
```

Once this is done, the API is aware of the namespaces you have access to: your profile and any organizations you are part of. To see the list of available commands, you can run `transformers-cli -h`, which will return something similar to:

```bash
usage: huggingface-cli <command> [<args>]

positional arguments:
  {login,whoami,logout,repo,lfs-enable-largefiles,lfs-multipart-upload}
                        huggingface-cli command helpers
    login               Log in using the same credentials as on huggingface.co
    whoami              Find out which huggingface.co account you are logged
                        in as.
    logout              Log out
    repo                {create, ls-files} Commands to interact with your
                        huggingface.co repos.
    lfs-enable-largefiles
                        Configure your repository to enable upload of files >
                        5GB.
    lfs-multipart-upload
                        Command will get called by git-lfs, do not call it
                        directly.

optional arguments:
  -h, --help            show this help message and exit
```

To upload a pretrained model, use the `repo create` command to create a new repository as follows:

```bash
huggingface-cli repo create dummy-model
```

This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `--organization` flag:

```bash
huggingface-cli repo create dummy-model --organization huggingface
```

This will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization. In the last part of this section, we'll show you how to upload files to your newly created repository

## Using the web interface

The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.

To create a new repository, visit [huggingface.co/new](https://huggingface.co/new):

<p align="center">
<img src="/course/static/chapter4/new_model.png" alt="Page showcasing the model used for the creation of a new model repository." width="80%"/>
</p>

First, specify the owner of the repository: this can be either you or any of the organizations you're affiliated with. If you choose an organization, the model will be featured on the organization's page and every member of the organization will have the ability to contribute to the repository.

Next, enter your model's name. This will also be the name of the repository. Finally, you can specify whether you want your model to be public or private. Private models require you to have a paid Hugging Face account, and will allow you have models hidden from public view.

After creating your model repository, you should see a page like this:

<p align="center">
<img src="/course/static/chapter4/empty_model.png" alt="An empty model page after creating a new repository." width="80%"/>
</p>

This is where your model will be hosted. To start populating it, you can add a README file directly from the web interface.

<p align="center">
<img src="/course/static/chapter4/dummy_model.png" alt="The README file showing the Markdown capabilities." width="80%"/>
</p>

The README file is in Markdown ‚Äî feel free to go wild with it! The third part of this chapter is dedicated to building a model card. These are of prime importance in bringing value to your model, as they're where you tell others what it can do.

If you look at the "Files and versions" tab, you'll see that there aren't many files there yet ‚Äî just the *README.md* you just created and the *.gitattributes* file that keeps track of large files.

<p align="center">
<img src="/course/static/chapter4/files.png" alt="The 'Files and versions' tab only shows the .gitattributes and README.md files." width="80%"/>
</p>

We'll take a look at how to add some new files next.

## Uploading the model files

The system to manage files on the Hugging Face Hub is based on `git` for regular files, and `git-lfs` (which stands for [Git Large File Storage](https://git-lfs.github.com/)) for larger files. Before continuing, please make sure you have both `git` and `git-lfs` installed on your system.

After you've downloaded and installed these tools, run `git lfs install` to initialize `git-lfs`:

```bash
git lfs install
```

```bash
Updated git hooks.
Git LFS initialized.
```

Once that's done, the first step is to clone your model repository:

```bash
git clone https://huggingface.co/<your-username>/<your-model-id>
```

My username is `lysandre` and I've used the model name `dummy`, so for me the command ends up looking like the following:

```
git clone https://huggingface.co/lysandre/dummy
```

I now have a folder named *dummy* in my working directory. I can `cd` into the folder and have a look at the contents:

```bash
cd dummy && ls
```

```bash
README.md
```

If you just created your repository using `push_to_hub` or the `transformers` CLI, this folder should be empty. If you followed the instructions in the previous section to create a repository using the web interface, the folder should contain a single *README.md* file, as shown here.

Adding a regular-sized file, such as a configuration file, a vocabulary file, or basically any file under a few megabytes, is done exactly as one would do it in any `git`-based system. However, bigger files must be registered through `git-lfs` in order to push them to *huggingface.co*. 

Let's go back to Python for a bit to generate a model and tokenizer that we'd like to commit to our dummy repository:

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Do whatever with the model, train it, fine-tune it...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Do whatever with the model, train it, fine-tune it...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{/if}

Now that we've saved some model and tokenizer artifacts, let's take another look at the *dummy* folder:

```bash
ls
```

{#if fw === 'pt'}
```bash
config.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json
```

If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*pytorch_model.bin*) is the only outlier, at more than 400 MB.

{:else}
```bash
config.json  README.md  sentencepiece.bpe.model  special_tokens_map.json  tf_model.h5  tokenizer_config.json  tokenizer.json
```

If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*t5_model.h5*) is the only outlier, at more than 400 MB.

{/if}

<Tip>
‚úèÔ∏è When creating the repository from the web interface, the *.gitattributes* file is automatically set up to consider files with certain extensions, such as *.bin* and *.h5*, as large files, and `git-lfs` will track them with no necessary setup on your side.
</Tip> 

We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:

```bash
git add .
```

We can then have a look at the files that are currently staged:

```bash
git status
```

{#if fw === 'pt'}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
	new file:   config.json
	new file:   pytorch_model.bin
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{:else}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
  	new file:   config.json
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tf_model.h5
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{/if}

Similarly, we can make sure that `git-lfs` is tracking the correct files by using its `status` command:

```bash
git lfs status
```

{#if fw === 'pt'}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	pytorch_model.bin (LFS: 35686c2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

We can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiece.bpe.model*, which have `LFS`. Great!

{:else}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tf_model.h5 (LFS: 86fce29)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

We can see that all files have `Git` as a handler, except *t5_model.h5*, which has `LFS`. Great!

{/if}

Let's proceed to the final steps, committing and pushing to the *huggingface.co* remote repository:

```bash
git commit -m "First model version"
```

{#if fw === 'pt'}
```bash
[main b08aab1] First model version
 7 files changed, 29027 insertions(+)
  6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 pytorch_model.bin
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{:else}
```bash
[main b08aab1] First model version
 6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tf_model.h5
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{/if}

Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:

```bash
git push
```

```bash
Uploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.
Enumerating objects: 11, done.
Counting objects: 100% (11/11), done.
Delta compression using up to 12 threads
Compressing objects: 100% (9/9), done.
Writing objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.
Total 9 (delta 1), reused 0 (delta 0), pack-reused 0
To https://huggingface.co/lysandre/dummy
   891b41d..b08aab1  main -> main
```

{#if fw === 'pt'}
If we take a look at the model repository when this is finished, we can see all the recently added files:

<p align="center">
<img src="/course/static/chapter4/full_model.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</p>

The UI allows you to explore the model files and commits and to see the diff introduced by each commit:

<p align="center">
<img src="/course/static/chapter4/diffs.gif" alt="The diff introduced by the recent commit." width="80%"/>
</p>
{:else}
If we take a look at the model repository when this is finished, we can see all the recently added files:

<p align="center">
<img src="/course/static/chapter4/full_model_tf.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</p>

The UI allows you to explore the model files and commits and to see the diff introduced by each commit:

<p align="center">
<img src="/course/static/chapter4/diffstf.gif" alt="The diff introduced by the recent commit." width="80%"/>
</p>
{/if}